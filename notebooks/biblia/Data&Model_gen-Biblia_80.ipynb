{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1122 20:48:23.615469 140287136569088 deprecation.py:323] From /store/tveiga/miniconda3/envs/py/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "# import tensorflow.compat.v1 as tf\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential, Input\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6502987162779171497\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6170955027291068975\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 18105614151092094428\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10989682688\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10626947068664170743\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Bíblia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('biblia.json') as f:\n",
    "    content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = json.loads(content[1:]) # carrega string em um json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chaps = []\n",
    "for book in j:\n",
    "    for chap in book['chapters']:\n",
    "        all_chaps.append(' '.join(chap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = []\n",
    "y_data = []\n",
    "for i in range(500_000):\n",
    "    c = np.random.randint(0, len(all_chaps))\n",
    "    curr_chap = all_chaps[c]\n",
    "    a = np.random.randint(0, len(curr_chap) - (SEQUENCE_LENGTH + 1))\n",
    "    \n",
    "    X_data.append(curr_chap[a : a + SEQUENCE_LENGTH])\n",
    "    y_data.append(curr_chap[a + SEQUENCE_LENGTH])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ue há em Cristo Jesus com glória eterna. Fiel é esta palavra: Se, pois, já morre - m\n",
      "as, vivamos no presente mundo sóbria, e justa, e piamente, aguardando a bem-aven - t\n",
      ". Partindo dali, fomos navegando a sotavento de Chipre, porque os ventos eram co - n\n",
      "vieram a Ezequias com as vestes rasgadas, e lhe fizeram saber as palavras de Rab - s\n",
      "rolados, da idade de vinte anos e acima, que foram seiscentos e três mil quinhen - t\n",
      " moças, como a irmãs, com toda a pureza. Honra as viúvas que são verdadeiramente -  \n",
      "o vinho de furor, e faze que dele bebam todas as nações, às quais eu te enviar.  - B\n",
      ", e favas, e lentilhas, e milho miúdo, e espelta, e mete-os numa só vasilha, e d - e\n",
      "uste será pior do que o primeiro. Disse-lhes Pilatos: Tendes uma guarda; ide, to - r\n",
      " abundância de suas delícias. Ouvi outra voz do céu dizer: Sai dela, povo meu, p - a\n"
     ]
    }
   ],
   "source": [
    "for dado, alvo in zip(X_data[:10], y_data[:10]):\n",
    "    print(dado, '-', alvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    num_words=500, # número máximo - valor grande arbitrário - se tirar não funciona - não sei pq\n",
    "    char_level=True,\n",
    "    filters=None,\n",
    "    lower=False,\n",
    "    oov_token=chr(1) # símbolo com quadradinho - funciona no js\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 89 unique tokens.\n",
      "CPU times: user 6.27 s, sys: 0 ns, total: 6.27 s\n",
      "Wall time: 6.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "tokenizer.fit_on_texts(y_data)\n",
    "# word_index = tokenizer.word_index\n",
    "# index_word = tokenizer.index_word\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_data_bibl40 = X_data\n",
    "# y_data_bibl40 = y_data\n",
    "# tok_bibl = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tokenizer.texts_to_sequences(X_data), dtype=np.int32)\n",
    "y = np.array(list(map(tokenizer.word_index.get, y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chars = len(tokenizer.index_word) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 320 # 256\n",
    "seq_units = 192 # 128 # faz muita diferença diminuir\n",
    "drop = .1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_chars, emb_dim, input_length=X_train.shape[1]))\n",
    "# model.add(SpatialDropout1D(drop)) # maybe should remove because of rare words...\n",
    "model.add(Bidirectional(GRU(seq_units, return_sequences=True, dropout=0.1,recurrent_dropout=0.1)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "\n",
    "model.add(Dense(num_chars, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 320)           28800     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 80, 384)           590976    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 90)                34650     \n",
      "=================================================================\n",
      "Total params: 654,426\n",
      "Trainable params: 654,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=RMSprop(lr=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 320000 samples, validate on 80000 samples\n",
      "Epoch 1/400\n",
      "320000/320000 [==============================] - 54s 169us/step - loss: 2.4540 - sparse_categorical_accuracy: 0.3093 - val_loss: 2.1755 - val_sparse_categorical_accuracy: 0.3649\n",
      "Epoch 2/400\n",
      "320000/320000 [==============================] - 53s 166us/step - loss: 2.0804 - sparse_categorical_accuracy: 0.3863 - val_loss: 2.0003 - val_sparse_categorical_accuracy: 0.4039\n",
      "Epoch 3/400\n",
      "320000/320000 [==============================] - 53s 166us/step - loss: 1.9428 - sparse_categorical_accuracy: 0.4185 - val_loss: 1.9029 - val_sparse_categorical_accuracy: 0.4276\n",
      "Epoch 4/400\n",
      "320000/320000 [==============================] - 53s 166us/step - loss: 1.8543 - sparse_categorical_accuracy: 0.4406 - val_loss: 1.8133 - val_sparse_categorical_accuracy: 0.4539\n",
      "Epoch 5/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.7865 - sparse_categorical_accuracy: 0.4598 - val_loss: 1.7576 - val_sparse_categorical_accuracy: 0.4682\n",
      "Epoch 6/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.7320 - sparse_categorical_accuracy: 0.4748 - val_loss: 1.7168 - val_sparse_categorical_accuracy: 0.4804\n",
      "Epoch 7/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.6874 - sparse_categorical_accuracy: 0.4883 - val_loss: 1.6746 - val_sparse_categorical_accuracy: 0.4955\n",
      "Epoch 8/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.6510 - sparse_categorical_accuracy: 0.4990 - val_loss: 1.6449 - val_sparse_categorical_accuracy: 0.5038\n",
      "Epoch 9/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.6190 - sparse_categorical_accuracy: 0.5085 - val_loss: 1.6179 - val_sparse_categorical_accuracy: 0.5070\n",
      "Epoch 10/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.5927 - sparse_categorical_accuracy: 0.5155 - val_loss: 1.5875 - val_sparse_categorical_accuracy: 0.5196\n",
      "Epoch 11/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.5699 - sparse_categorical_accuracy: 0.5216 - val_loss: 1.5657 - val_sparse_categorical_accuracy: 0.5261\n",
      "Epoch 12/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.5500 - sparse_categorical_accuracy: 0.5267 - val_loss: 1.5523 - val_sparse_categorical_accuracy: 0.5307\n",
      "Epoch 13/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.5326 - sparse_categorical_accuracy: 0.5308 - val_loss: 1.5359 - val_sparse_categorical_accuracy: 0.5335\n",
      "Epoch 14/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.5159 - sparse_categorical_accuracy: 0.5355 - val_loss: 1.5243 - val_sparse_categorical_accuracy: 0.5365\n",
      "Epoch 15/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.5012 - sparse_categorical_accuracy: 0.5395 - val_loss: 1.5168 - val_sparse_categorical_accuracy: 0.5391\n",
      "Epoch 16/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.4880 - sparse_categorical_accuracy: 0.5429 - val_loss: 1.4991 - val_sparse_categorical_accuracy: 0.5420\n",
      "Epoch 17/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.4763 - sparse_categorical_accuracy: 0.5460 - val_loss: 1.4868 - val_sparse_categorical_accuracy: 0.5477\n",
      "Epoch 18/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.4651 - sparse_categorical_accuracy: 0.5484 - val_loss: 1.4864 - val_sparse_categorical_accuracy: 0.5489\n",
      "Epoch 19/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.4534 - sparse_categorical_accuracy: 0.5519 - val_loss: 1.4801 - val_sparse_categorical_accuracy: 0.5500\n",
      "Epoch 20/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.4437 - sparse_categorical_accuracy: 0.5546 - val_loss: 1.4638 - val_sparse_categorical_accuracy: 0.5552\n",
      "Epoch 21/400\n",
      "320000/320000 [==============================] - 53s 167us/step - loss: 1.4341 - sparse_categorical_accuracy: 0.5572 - val_loss: 1.4645 - val_sparse_categorical_accuracy: 0.5568\n",
      "CPU times: user 24min 31s, sys: 2min 37s, total: 27min 9s\n",
      "Wall time: 18min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n = len(X_train) // 1\n",
    "# change val split to .1 or .05?\n",
    "\n",
    "epochs = 400\n",
    "batch_size = 1024\n",
    "history = model.fit(X_train[:n], y_train[:n], epochs=epochs, batch_size=batch_size,\n",
    "                    validation_split=0.2,\n",
    "#                     sample_weight = sample_weight[:n],\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss',patience=1, min_delta=1e-7)],\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('biblia_80-keras_model.h5')\n",
    "# pickle.dump(history, open(\"biblia-history.p\", \"wb\"))\n",
    "with open('biblia_80-index_word.json', 'w') as f:\n",
    "    json.dump(tokenizer.index_word, f, ensure_ascii=False)\n",
    "with open('biblia_80-word_index.json', 'w') as f:\n",
    "    json.dump(tokenizer.word_index, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('biblia-word_index.json', 'r') as f:\n",
    "#     x = json.load(f)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('keras_model.h5')\n",
    "# history = pickle.load(open(\"history.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = X_test[0].copy().reshape((1, -1))\n",
    "original = ''.join(map(tokenizer.index_word.get, curr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_seq = []\n",
    "while True:\n",
    "    _next = model.predict_classes(curr)[0]\n",
    "    next_char = tokenizer.index_word[_next]\n",
    "    if next_char == ' ':\n",
    "        break\n",
    "    next_seq.append(next_char)\n",
    "    curr[0, 0 : -1] = curr[0, 1 :]\n",
    "    curr[0, -1] = _next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " se apressou a sair, porque o Senhor o ferira. Assim ficou leproso o rei Uzias a \n"
     ]
    }
   ],
   "source": [
    "print(original, ''.join(next_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " se apressou a sair, porque o Senhor o ferira. Assim ficou leproso o rei Uzias a --  sua mão de Deus. E\n",
      " mulher tiver o cabelo comprido, é para ela uma glória? Pois a cabeleira lhe foi --  de todos os que estavam\n",
      " suceda que morra na peleja e outro a receba. Assim continuarão os oficiais a fa -- zer do Senhor do Senhor dos\n",
      "mês, no décimo quinto ano do reinado de Asa. E no mesmo dia ofereceram em sacrif -- ícios de Israel, e os filhos\n",
      "uja força era como a dos carvalhos; mas destruí o seu fruto por cima, e as suas  -- servos de todos os seus servos\n",
      "iverem todos eles prontos nos teus lábios. Para que a tua confiança esteja no se -- u contra o Senhor, e o\n",
      "do todos estes jóias de ouro; assim veio todo aquele que queria fazer oferta de  -- meu povo, e o seu por\n",
      "o necessitado, daquele que o rouba. Levantam-se testemunhas maliciosas; interrog -- ar a minha mão de Deus.\n",
      "az o coração dos tolos. O sacrifício dos ímpios é abominável ao Senhor; mas a or -- denação de todos os filhos de\n",
      " do que todos os que houve antes de mim em Jerusalém. Ajuntei também para mim pr -- ofeta com o Senhor de Deus,\n",
      "e o teu povo e sobre a casa de teu pai, dias tais, quais nunca vieram, desde o d -- e todos os que estavam a\n",
      " e matastes o Autor da vida, a quem Deus ressuscitou dentre os mortos, do que nó -- s de Deus, e os seus\n",
      "Israel: Não convém que vós e nós edifiquemos casa a nosso Deus: mas nós sozinhos --  de Deus, e o seu\n",
      "mão e ferido a ti e ao teu povo com pestilência, e tu terias sido destruído da t -- erra de Deus. E o seu\n",
      "m por todos os dias da tua vida, e verás os filhos de teus filhos. A paz seja so -- bre o meu povo do Senhor\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    curr = X_test[i].copy().reshape((1, -1))\n",
    "    original = ''.join(map(tokenizer.index_word.get, curr[0]))\n",
    "    blank_count = 0\n",
    "    next_seq = []\n",
    "    while True:\n",
    "        _next = model.predict_classes(curr)[0]\n",
    "        next_char = tokenizer.index_word[_next]\n",
    "        if next_char == ' ':\n",
    "            blank_count += 1\n",
    "        if blank_count == 6:\n",
    "            break\n",
    "        next_seq.append(next_char)\n",
    "        curr[0, 0 : -1] = curr[0, 1 :]\n",
    "        curr[0, -1] = _next\n",
    "        \n",
    "    print(original, '--', ''.join(next_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
