{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "Cb3ztjI91g4g",
    "outputId": "8105d586-494e-47c8-d2ed-85efac26273a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(42)\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout, RepeatVector\n",
    "from keras.optimizers import RMSprop\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import sys\n",
    "import heapq\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v0u8O2sg1x9D"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential, Input\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "# SEQUENCE SIZE PARAMETERS\n",
    "CHARACTER_NUMBER_PREDICTION = 80\n",
    "DATA_SET_COLLECTION_ITERATIONS = 50_000\n",
    "SET_VARIABILITY = 3\n",
    "\n",
    "# RNN PARAMETERS\n",
    "EMB_DIM = 256\n",
    "SEQ_UNITS = 256\n",
    "DROP = .1\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "TEST_SIZE = 0.2\n",
    "EPOCHS = 400\n",
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = .2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ht9k06l411rL"
   },
   "outputs": [],
   "source": [
    "contentDf = pd.read_csv('bras_cubas_paragraphs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7bzrODFBKsA"
   },
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):\n",
    "\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3 \n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WKBLSpHGBWIO"
   },
   "outputs": [],
   "source": [
    "# Arrays we'll use to store our dataset\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "# Looping through our corpus...\n",
    "for i in range(DATA_SET_COLLECTION_ITERATIONS):\n",
    "    # ... selecting a random paragraph from our book...\n",
    "    paragraphIndex = np.random.randint(0, len(contentDf.paragraphs))\n",
    "    currentParagraph = contentDf.paragraphs[paragraphIndex]\n",
    "  \n",
    "    # ... sampling a slice of the selected paragraph...\n",
    "    #   aux = 1\n",
    "    #   dontAddThisSample = False\n",
    "    #   while(len(currentParagraph) - CHARACTER_NUMBER_PREDICTION + 1 < CHARACTER_NUMBER_PREDICTION + 1):\n",
    "    #     # currentParagraph += contentDf.paragraphs[np.random.randint(0, len(contentDf.paragraphs))]\n",
    "    #     if (paragraphIndex + aux < len(contentDf.paragraphs)):\n",
    "    #         currentParagraph += contentDf.paragraphs[paragraphIndex + aux]\n",
    "    #         aux += 1\n",
    "    #     else:\n",
    "    #         dontAddThisSample = True\n",
    "    #         break;\n",
    "        \n",
    "    #   if (dontAddThisSample):\n",
    "    #     continue\n",
    "\n",
    "    if (len(currentParagraph) < CHARACTER_NUMBER_PREDICTION + 1):\n",
    "        continue\n",
    "        \n",
    "        \n",
    "    \n",
    "    paragraphRegion = np.random.randint(0, len(currentParagraph) - CHARACTER_NUMBER_PREDICTION)\n",
    "    \n",
    "    # Checking how many different chars are in the selected paragraph region\n",
    "    nChars = len(set(currentParagraph[paragraphRegion : paragraphRegion + CHARACTER_NUMBER_PREDICTION]))\n",
    "    \n",
    "    if (nChars < SET_VARIABILITY):\n",
    "        continue\n",
    "  \n",
    "    \n",
    "    # Adding an excerpt of the paragraph to our X and y data.\n",
    "    X_data.append(currentParagraph[paragraphRegion : paragraphRegion + CHARACTER_NUMBER_PREDICTION].casefold())\n",
    "    y_data.append(currentParagraph[paragraphRegion + CHARACTER_NUMBER_PREDICTION].casefold())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31758, 31758)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_data), len(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qFbfudiPN5MP"
   },
   "source": [
    "In the cells below, we'll instantiate and fit a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xVFYTOMkHjGJ"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(\n",
    "    num_words=500,\n",
    "    char_level=True,\n",
    "    filters=None,\n",
    "    lower=False,\n",
    "    oov_token=chr(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "h1ml7sn-LmvV",
    "outputId": "846ab1e4-0915-4bbf-ade7-e530ccffd60b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 397 ms, sys: 8 µs, total: 397 ms\n",
      "Wall time: 397 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer.fit_on_texts(X_data)\n",
    "tokenizer.fit_on_texts(y_data)\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hs3LJUSBL8i0"
   },
   "outputs": [],
   "source": [
    "X = np.array(tokenizer.texts_to_sequences(X_data), dtype=np.int32)\n",
    "y = np.array(list(map(word_index.get, y_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "737CDqaGMkjf",
    "outputId": "a2fcdcf8-a09e-4a13-eb75-f18584ae39a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  7  3 ...  5  2 23]\n",
      " [22 10  9 ...  2 19  4]\n",
      " [13  8  5 ...  2 12  7]\n",
      " ...\n",
      " [ 2 18  4 ... 15  4 22]\n",
      " [13  4  2 ...  6  4  2]\n",
      " [10  4  2 ...  9  3  7]]\n",
      "[4 7 6 ... 8 3 6]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eeTG9ePCOuPA",
    "outputId": "419dcf5e-c5e4-41de-a313-593c8f3bfded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (25406, 80), Y train shape : (25406,), X test shape: (6352, 80), Y test shape: (6352,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)\n",
    "print(f\"X train shape: {X_train.shape}, Y train shape : {y_train.shape}, X test shape: {X_test.shape}, Y test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "X_VM3plzRsoQ",
    "outputId": "73a32fbe-5fd2-4e74-bfd3-4fe2256eaded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PmeWLbt-OMnZ"
   },
   "source": [
    "# Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EJZVax2mOO0T"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, EMB_DIM, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(DROP))\n",
    "model.add(Bidirectional(GRU(SEQ_UNITS, return_sequences=True, dropout=DROP,recurrent_dropout=DROP)))\n",
    "model.add(SpatialDropout1D(DROP))\n",
    "model.add(Bidirectional(GRU(SEQ_UNITS, return_sequences=True, dropout=DROP,recurrent_dropout=DROP)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(len(tokenizer.word_index), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "PPwvhgBOP9LX",
    "outputId": "41b66a9e-0aa6-48af-fde5-a9ca5d7b258a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 80, 256)           17152     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, 80, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 80, 512)           787968    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 80, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 80, 512)           1181184   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 66)                33858     \n",
      "=================================================================\n",
      "Total params: 2,020,162\n",
      "Trainable params: 2,020,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=RMSprop(lr=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "M7TKfzAYQOxj",
    "outputId": "6627de62-d5f4-4dc7-d664-9d5eb2473bbf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20324 samples, validate on 5082 samples\n",
      "Epoch 1/400\n",
      "20324/20324 [==============================] - 19s 916us/step - loss: 2.9607 - sparse_categorical_accuracy: 0.1955 - val_loss: 2.6185 - val_sparse_categorical_accuracy: 0.2497\n",
      "Epoch 2/400\n",
      "20324/20324 [==============================] - 16s 807us/step - loss: 2.4487 - sparse_categorical_accuracy: 0.2866 - val_loss: 2.4132 - val_sparse_categorical_accuracy: 0.2712\n",
      "Epoch 3/400\n",
      "20324/20324 [==============================] - 16s 805us/step - loss: 2.2744 - sparse_categorical_accuracy: 0.3206 - val_loss: 2.3306 - val_sparse_categorical_accuracy: 0.2684\n",
      "Epoch 4/400\n",
      "20324/20324 [==============================] - 16s 805us/step - loss: 2.1879 - sparse_categorical_accuracy: 0.3372 - val_loss: 2.2525 - val_sparse_categorical_accuracy: 0.3074\n",
      "Epoch 5/400\n",
      "20324/20324 [==============================] - 16s 807us/step - loss: 2.1215 - sparse_categorical_accuracy: 0.3523 - val_loss: 2.1739 - val_sparse_categorical_accuracy: 0.3306\n",
      "Epoch 6/400\n",
      "20324/20324 [==============================] - 16s 807us/step - loss: 2.0596 - sparse_categorical_accuracy: 0.3702 - val_loss: 2.1120 - val_sparse_categorical_accuracy: 0.3530\n",
      "Epoch 7/400\n",
      "20324/20324 [==============================] - 16s 808us/step - loss: 1.9916 - sparse_categorical_accuracy: 0.3871 - val_loss: 2.0935 - val_sparse_categorical_accuracy: 0.3613\n",
      "Epoch 8/400\n",
      "20324/20324 [==============================] - 16s 806us/step - loss: 1.9243 - sparse_categorical_accuracy: 0.4043 - val_loss: 2.0484 - val_sparse_categorical_accuracy: 0.3739\n",
      "Epoch 9/400\n",
      "20324/20324 [==============================] - 16s 808us/step - loss: 1.8598 - sparse_categorical_accuracy: 0.4217 - val_loss: 2.0398 - val_sparse_categorical_accuracy: 0.3835\n",
      "Epoch 10/400\n",
      "20324/20324 [==============================] - 16s 808us/step - loss: 1.7871 - sparse_categorical_accuracy: 0.4483 - val_loss: 1.9889 - val_sparse_categorical_accuracy: 0.3975\n",
      "Epoch 11/400\n",
      "20324/20324 [==============================] - 16s 807us/step - loss: 1.7105 - sparse_categorical_accuracy: 0.4680 - val_loss: 2.0085 - val_sparse_categorical_accuracy: 0.3987\n",
      "CPU times: user 7min 3s, sys: 36.1 s, total: 7min 39s\n",
      "Wall time: 3min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=EPOCHS, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss',patience=1,\n",
    "                                            min_delta=1e-7)]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = X_test[0].copy().reshape((1, -1))\n",
    "original = ''.join(map(tokenizer.index_word.get, curr[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_seq = []\n",
    "while True:\n",
    "    _next = model.predict_classes(curr)[0]\n",
    "    next_char = tokenizer.index_word[_next]\n",
    "    if next_char == ' ':\n",
    "        break\n",
    "    next_seq.append(next_char)\n",
    "    curr[0, 0 : -1] = curr[0, 1 :]\n",
    "    curr[0, -1] = _next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seriam tenues, e compradas a troco da solidão. sem filhos! não; impossivel. dis se\n"
     ]
    }
   ],
   "source": [
    "print(original, ''.join(next_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  6,  4,  8,  7,  3,  9,  2, 13,  4, 11, 10,  4,  6, 16,  2,  4,\n",
       "        2, 14,  5,  9, 17,  8,  3, 12,  3,  6,  2,  3,  2, 13,  8,  5, 14,\n",
       "        5,  2, 12,  3,  2,  6,  5, 15,  7, 12, 25,  5, 20,  2,  6,  4,  9,\n",
       "        2, 24,  7, 15, 19,  5,  6, 35,  2, 11, 25,  5, 27,  2,  7,  9, 17,\n",
       "        5,  6,  6,  7, 18,  4, 15, 20,  2, 12,  7,  6], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " seriam tenues, e compradas a troco da solidão. sem filhos! não; impossivel. dis -- se e pera verte e pera\n",
      "deiramente christão. todavia, não neguei aos amigos as vantagens pecuniarias que --  fante a menha vertigo e\n",
      "stume jantava ahi; mas, não tendo deliberadamente andado, nenhum merecimento da  -- meneito de meite de memente com\n",
      "obrigado a aceitar as duas; creio que posso ser separadamente homem casado ou ho -- mente de meito de conteiro, e\n",
      "os famulos, que naturalmente se desforravam assim da condição servil, e tudo iss -- e e a menos de por\n",
      "sentar ao pé da filha do damasceno, uma d. eulalia, ou mais familiarmente nhã-ló --  e eu conteito de perteiro\n",
      "te, ao menos, uma parte cheia de prazeres, de agitações, de sustos,— capeada de  -- porte em porte de memos e\n",
      " eu nasci, estava já em todo o explendor da gloria e do poder; era imperador e g -- ranha para de canteira cama cantava\n",
      " o humanitismo; elle é o grande regaço dos espiritos, o mar eterno em que mergul -- a e eu pera e eu\n",
      " indifferença. eu mesmo, até então, tinha-vos em má conta, zangava-me quando vos -- te e pereceiro. e menei e\n",
      ". não importa a edade do adulado; a mulher ha de ter sempre para elle uns ares d -- e porteito de conteiro de paraçava\n",
      "incas borba fez-me parar e observar os cães. eram dous. notou que ao pé delles e --  porte de consisse para com\n",
      "o, ponderou elle, no fim de um instante, mas ninguem me negará sentimento, se nã -- o canha para per para de\n",
      "u-me afflicta. que idéa é essa de provocar o governo, sem necessidade, quando po -- r e veria per pera de\n",
      "de de amor era a mesma; a differença é que a chamma perdera o tresloucado dos pr -- eceteiro de porte de perta de\n",
      "o, disse eu, arranjei recursos...temos muito dinheiro, terás tudo o que quizeres -- .... não e temporia e por\n",
      "beira das calças,— umas largas calças de enfiar— , ou na gaveta da mesa, ou ao p -- ades a conteiro de menho de\n",
      ". não receies perder esse andrajo que é teu orgulho; provarás ainda, por algumas --  com o perdida de menos\n",
      "temos a este proposito uma anecdota. foi no tempo da minha vida parlamentar; era --  menei a menos para a\n",
      "adia! murmurou elle sem responder ao pedido. um cadaver... o mar... o ceu... o n -- o peria... me verte de mente\n",
      "ebi-o sem alvoroço. elle esteve alguns instantes de pé, a olhar para mim; depois --  de podia de para de\n",
      "o grave, e aliás infimo, porque o maior defeito deste livro és tu, leitor. tu te -- mporia o porte de perte a\n",
      "illaça, glosador insigne, que accrescentou aos pratos de casa o acepipe das musa --  de porte de menos e\n",
      ", porque a candidatura de lobo neves era apoiada por grandes influencias. cedi;  -- e parecia de cousa de paraça\n",
      "sta de primavera, um amanhecer da alma publica. eramos dous rapazes, o povo e eu --  conteito de menos e meu\n",
      "o nas mãos, e o espirito ainda mais cabisbaixo do que a figura,— ou jururú, como --  este e menos e menos\n",
      "as cousas externas, embelleza-se no invisivel, apprehende o impalpavel, desvincu -- ei a porteiro de meito de\n",
      "steza tornou logo, a tristeza de morrer sem me ver posto em algum logar alto, co -- m o conteiro. não e e\n",
      " não sei... são bem frouxos versos. jurei-lhe que não; pedi que os reunisse e me -- nei e conteiro e menei e\n",
      " ponto, que foi preciso deital-o á margem, onde o realismo o veiu achar, comido  -- a menos... não e menos.. ponte\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    curr = X_test[i].copy().reshape((1, -1))\n",
    "    original = ''.join(map(tokenizer.index_word.get, curr[0]))\n",
    "    blank_count = 0\n",
    "    next_seq = []\n",
    "    while True:\n",
    "        _next = model.predict_classes(curr)[0]\n",
    "        next_char = tokenizer.index_word[_next]\n",
    "        if next_char == ' ':\n",
    "            blank_count += 1\n",
    "        if blank_count == 6:\n",
    "            break\n",
    "        next_seq.append(next_char)\n",
    "        curr[0, 0 : -1] = curr[0, 1 :]\n",
    "        curr[0, -1] = _next\n",
    "        \n",
    "    print(original, '--', ''.join(next_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('bras_cubas_80-keras_model.h5')\n",
    "with open('bras_cubas_80-index_word.json', 'w') as f:\n",
    "    json.dump(tokenizer.index_word, f, ensure_ascii=False)\n",
    "with open('bras_cubas_80-word_index.json', 'w') as f:\n",
    "    json.dump(tokenizer.word_index, f, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "bras-cubas-model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
